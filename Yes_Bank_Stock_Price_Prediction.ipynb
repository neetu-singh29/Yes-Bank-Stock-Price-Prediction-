{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "OH-pJp9IphqM",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member -** Neetu Singh\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:** The Yes Bank Stock Price Prediction project aims to develop an effective machine learning model to forecast stock prices using historical data. Stock price prediction is a challenging task due to the volatility of financial markets, the influence of multiple economic factors, and the impact of external events. By leveraging data science techniques, this project attempts to build predictive models that help investors and traders make informed decisions.\n",
        "\n",
        "**Objective:**\n",
        "The primary objective of this project is to use machine learning algorithms to analyze historical stock price data of Yes Bank and predict future stock trends. The project evaluates different regression models to determine the most accurate model for forecasting. The models used include:\n",
        "âœ… Linear Regression\n",
        "âœ… Random Forest Regressor\n",
        "âœ… Gradient Boosting Regressor\n",
        "\n",
        "These models are assessed based on performance metrics like Mean Squared Error (MSE), R-squared (RÂ²), and Mean Absolute Error (MAE). Additionally, hyperparameter tuning techniques such as GridSearchCV and RandomizedSearchCV are employed to optimize the models for better predictions.\n",
        "\n",
        "**Steps Involved in the Project:**\n",
        "\n",
        "ðŸ”¹ **Data Collection & Preprocessing:**\n",
        "\n",
        "The dataset includes historical stock prices of Yes Bank with features like Open, High, Low, Close, and Volume.\n",
        "Data cleaning steps such as handling missing values, outliers, and feature selection are performed.\n",
        "The dataset is split into training and testing sets for model development.\n",
        "\n",
        "ðŸ”¹ **Exploratory Data Analysis (EDA):**\n",
        "\n",
        "Statistical analysis of stock price movements is conducted.\n",
        "Visualizations like time series plots, correlation heatmaps, and moving averages are used to understand trends and patterns.\n",
        "\n",
        "ðŸ”¹ **Model Implementation & Evaluation:**\n",
        "\n",
        "âœ… **Linear Regression:**\n",
        "\n",
        "A simple model used as a baseline for comparison.\n",
        "Performance:\n",
        "\n",
        "MSE: 0.1114\n",
        "\n",
        "RÂ²: 0.5470\n",
        "\n",
        "MAE: 0.2687\n",
        "\n",
        "Hyperparameter tuning was done using GridSearchCV, but the model did not show significant improvement.\n",
        "\n",
        "âœ… **Random Forest Regressor:**\n",
        "\n",
        "A more advanced ensemble model to capture non-linear relationships.\n",
        "\n",
        "Performance:\n",
        "\n",
        "MSE: 0.1339\n",
        "\n",
        "RÂ²: 0.4556\n",
        "\n",
        "MAE: 0.1575\n",
        "\n",
        "RandomizedSearchCV was used to fine-tune hyperparameters, leading to improved accuracy.\n",
        "\n",
        "âœ… **Gradient Boosting Regressor:**\n",
        "\n",
        "A boosting algorithm that improves predictions through iterative learning.\n",
        "\n",
        "Performance:\n",
        "\n",
        "MSE: 0.1369\n",
        "\n",
        "RÂ²: 0.4436\n",
        "\n",
        "MAE: 0.1505\n",
        "\n",
        "GridSearchCV was applied for hyperparameter optimization, enhancing results slightly.\n",
        "\n",
        "ðŸ”¹ **Model Explainability & Feature Importance:**\n",
        "\n",
        "  * Feature importance analysis using SHAP (SHapley Additive Explanations) and permutation importance.\n",
        "\n",
        "  * Identified key factors affecting stock prices, such as opening price, previous day's close, and trading volume.\n",
        "\n",
        "**Conclusion:**\n",
        "This project demonstrates the effectiveness of machine learning techniques in predicting stock prices. The ***Linear Regression model performed the best*** among the tested models in terms of RÂ², but ensemble models like ***Random Forest and Gradient Boosting provided more robust predictions*** by capturing complex patterns in the data.\n",
        "\n",
        "While the predictions are useful, stock markets are highly dynamic, and external factors like economic policies, global financial trends, and investor sentiment can significantly impact prices.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement :**\n",
        "Stock market investments require careful analysis of trends and price movements. Investors and traders face challenges in predicting future stock prices due to market volatility. This project aims to develop a **machine learning-based predictive model** for Yes Bankâ€™s stock price, utilizing historical data and various regression algorithms.\n",
        "\n",
        "The **objective** is to determine which model provides the **most accurate and reliable predictions** to aid investors in making informed decisions."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Display the first few rows\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Dataset Contain - Rows: {df.shape[0]}, Columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Check data information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicate rows: {df[df.duplicated()].shape[0]}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "# print(df.isnull().sum())\n",
        "\n",
        "# Missing Values/Null Values Count\n",
        "for df, name in [(df, 'df')]:\n",
        "    print(f\"Missing Values/Null Values Count for {name}:\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    display(missing_values)\n",
        "\n",
        "    total_missing = missing_values.sum()\n",
        "    print(f\"\\nTotal missing values in {name}: {total_missing}\\n\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(),cbar=False,cmap='viridis')\n",
        "plt.title(\"Missing value in Dataframe\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Based on the code execution, here's what we know about the\n",
        "\n",
        "**Yes Bank Stock Prices dataset:**\n",
        "\n",
        "**1. Shape:** The dataset has 185 rows and 5 columns. This means it contains 185 records of stock price information, each with 5 attributes.\n",
        "\n",
        "**2. Columns:** The columns are 'Date', 'Open', 'High', 'Low', and 'Close'. These represent the date of the record, the opening price, the highest price, the lowest price, and the closing price of Yes Bank stock for that day, respectively.\n",
        "\n",
        "**3. Data Types:** The 'Date' column is of object type (likely string), while the other columns ('Open', 'High', 'Low', 'Close') are of float64 type, representing numerical values.\n",
        "\n",
        "**4. Missing Values:** There are no missing values in any of the columns. This is indicated by the output of df.isnull().sum(), which shows 0 missing values for each column.\n",
        "\n",
        "**5. Duplicate Values:** There are no duplicate rows in the dataset, as confirmed by the output of df[df.duplicated()].shape[0].\n",
        "\n",
        "**6. Summary Statistics:** The df.describe() function provides descriptive statistics for the numerical columns, including count, mean, standard deviation, minimum, quartiles, and maximum. This gives us an initial overview of the distribution of the stock prices."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "df_columns = df.columns\n",
        "print(df_columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Display summary statistics\n",
        "display(df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Here's a description of each variable in the dataset:\n",
        "\n",
        "**1. Date:** The date of the stock price record. This is likely the primary key for the dataset.\n",
        "\n",
        "**2. Open:** The opening price of Yes Bank stock on that date.\n",
        "\n",
        "**3. High:** The highest price reached by Yes Bank stock during that trading day.\n",
        "\n",
        "**4. Low:** The lowest price reached by Yes Bank stock during that trading day.\n",
        "\n",
        "**5. Close:** The closing price of Yes Bank stock on that date. This is often considered the most important price for daily stock analysis.\n",
        "\n",
        "These variables provide the fundamental information needed to analyze the historical performance of Yes Bank stock and potentially build a predictive model for future prices."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Check Unique Values for each variable in df and display the output\n",
        "print(\"Unique values in df:\")\n",
        "display(df.apply(pd.unique))"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#ðŸ”µ 1. Date Handling\n",
        "\n",
        "# Check if 'Date' is already the index (to avoid modifying it multiple times)\n",
        "if 'Date' in df.columns:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%b-%d', errors='coerce')  # Convert to datetime, handle errors\n",
        "    df.set_index('Date', inplace=True)  # Set 'Date' as index\n",
        "\n",
        "\n",
        "#ðŸ”µ 2. Feature Engineering\n",
        "df['Daily_Change'] = df['Close'] - df['Open']\n",
        "df['Daily_Percent_Change'] = (df['Daily_Change'] / df['Open']) * 100\n",
        "df['5_Day_MA'] = df['Close'].rolling(window=5).mean()\n",
        "df['20_Day_MA'] = df['Close'].rolling(window=20).mean()\n",
        "# ... (Add more features if needed) ...\n",
        "\n",
        "\n",
        "#ðŸ”µ 3. Handling Missing Values\n",
        "#âœ… Check for missing values\n",
        "print(df.isnull().sum())\n",
        "# If missing values are present, use appropriate imputation techniques:\n",
        "# df['column_name'].fillna(df['column_name'].mean(), inplace=True)  # For numerical features\n",
        "# df['column_name'].fillna(df['column_name'].mode()[0], inplace=True)  # For categorical features\n",
        "'''# 3. Handling Missing Values in Moving Averages\n",
        "df['5_Day_MA'].fillna(method='ffill', inplace=True)\n",
        "df['20_Day_MA'].fillna(method='ffill', inplace=True)\n",
        "'''\n",
        "\n",
        "#ðŸ”µ 4. Handling Outliers\n",
        "# Using IQR method for 'Daily_Change'\n",
        "Q1 = df['Daily_Change'].quantile(0.25)\n",
        "Q3 = df['Daily_Change'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "df = df[(df['Daily_Change'] >= lower_bound) & (df['Daily_Change'] <= upper_bound)]\n",
        "\n",
        "\n",
        "#ðŸ”µ 5. Data Transformation (if needed)\n",
        "\n",
        "#âœ… Check for Skewness\n",
        "import scipy.stats as stats\n",
        "# Visualize distributions using histograms\n",
        "for feature in ['Daily_Change', 'Daily_Percent_Change']:  # Features to check\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate skewness\n",
        "    skewness = stats.skew(df[feature])\n",
        "    print(f'Skewness of {feature}: {skewness}')\n",
        "\n",
        "    # Check for normality using Shapiro-Wilk test\n",
        "    shapiro_statistic, shapiro_p_value = stats.shapiro(df[feature])\n",
        "    print(f\"Shapiro-Wilk Test for {feature}: Statistic={shapiro_statistic}, p-value={shapiro_p_value}\")\n",
        "    if shapiro_p_value < 0.05:\n",
        "        print(f\"Reject null hypothesis: {feature} is not normally distributed.\")\n",
        "    else:\n",
        "        print(f\"Fail to reject null hypothesis: {feature} may be normally distributed.\")\n",
        "    print(\"\\n\")  # Add newline for better separation\n",
        "\n",
        "\n",
        "\n",
        "#âœ… If any features have skewed distributions, apply transformations like log or Box-Cox or Yeo-Johnson.\n",
        "\n",
        "# ðŸ…° Log Transformation (Handle zero and negative values before log transformation)\n",
        "epsilon = 1e-8\n",
        "df['Daily_Change'] = df['Daily_Change'].apply(lambda x: epsilon if x <= 0 else x)\n",
        "df['Daily_Change'] = np.log1p(df['Daily_Change'])  # Log transformation for Daily_Change\n",
        "\n",
        "# ðŸ…± Yeo-Johnson Transformation for Daily_Percent_Change (No need to handle negative values for Yeo-Johnson)\n",
        "df['Daily_Percent_Change'], _ = stats.yeojohnson(df['Daily_Percent_Change'])\n",
        "\n",
        "\n",
        "print(df.describe())\n",
        "# ... (Create scatter plots for other features vs. Close) ...\n",
        "# Assess Linearity (using scatter plots)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Daily_Change'], df['Close'])  # Example scatter plot\n",
        "plt.title('Daily Change vs. Close Price')\n",
        "plt.xlabel('Daily Change')\n",
        "plt.ylabel('Close Price')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#ðŸ”µ 6. Data Scaling (if needed)\n",
        "# If using algorithms sensitive to feature scales (e.g., Linear Regression), apply scaling.\n",
        "# Select numerical features for scaling\n",
        "numerical_features = ['Open', 'High', 'Low', 'Close', 'Daily_Change', 'Daily_Percent_Change', '5_Day_MA', '20_Day_MA']\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Choose a scaler (StandardScaler)\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()  # Alternatively, use MinMaxScaler\n",
        "\n",
        "# Fit and transform the selected features\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "#ðŸ”µ 7. Data Splitting (for model building)\n",
        "# X = df[['Open', 'High', 'Low', 'Daily_Change', 'Daily_Percent_Change', '5_Day_MA', '20_Day_MA']]  # Select features for prediction\n",
        "# y = df['Close']  # Target variable\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Print the cleaned dataset (optional)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** âœ… **Data Wrangling Steps:**\n",
        "\n",
        "ðŸ”¹ **1. Date Handling :** Converted the 'Date' column to datetime format and set it as the index.\n",
        "Ensured that the date format was handled properly.\n",
        "\n",
        "ðŸ”¹ **2. Feature Engineering:** Created new features for better insights:\n",
        "\n",
        "* **Daily_Change:** Difference between Close and Open price.\n",
        "* **Daily_Percent_Change:** Percentage change in stock price (price during the day).\n",
        "* **5_Day_MA:** 5-day moving average of the closing price.\n",
        "* **20_Day_MA:** 20-day moving average of the closing price.\n",
        "\n",
        "ðŸ”¹ **3. Handling Missing Values:**\n",
        "* Identified missing values in the dataset.\n",
        "* Forward filled missing values in 5_Day_MA and 20_Day_MA columns.\n",
        "\n",
        "ðŸ”¹ **4. Handling Outliers:** Used Interquartile Range (IQR) to filter out extreme values from Daily_Change.\n",
        "\n",
        "ðŸ”¹ **5. Data Transformation:**\n",
        "* Checked for skewness in Daily_Change and Daily_Percent_Change.\n",
        "* Applied log transformation to Daily_Change to reduce skewness.This helps to reduce skewness and make the distribution more normal.\n",
        "* Applied Yeo-Johnson transformation to Daily_Percent_Change for normality.\n",
        "\n",
        "ðŸ”¹ **6. Data Scaling:**Standardized numerical features using StandardScaler.\n",
        "\n",
        "ðŸ”¹ **7. Data Splitting:**\n",
        "* Prepared data for model building:\n",
        " 1. **Features (X):** Open, High, Low, Daily_Change, Daily_Percent_Change, 5_Day_MA, 20_Day_MA\n",
        " 2. **Target (y):** Close\n",
        "* **Split Data:** 80% training, 20% testing.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ“Š **Insights from Data:**\n",
        "\n",
        "1ï¸âƒ£ **Stock Price Fluctuations:**Large daily price changes were observed, which may indicate volatility.\n",
        "Daily percent changes had extreme values (ranging from -222% to 155%).\n",
        "\n",
        "2ï¸âƒ£ **Moving Averages:**\n",
        "* The 5-day moving average fluctuates more than the 20-day moving average.\n",
        "\n",
        "* The 20-day moving average indicates long-term price trends.\n",
        "\n",
        "3ï¸âƒ£ **Skewness & Normality:**\n",
        "* Daily_Change was slightly left-skewed, so log transformation was applied.\n",
        "\n",
        "* Daily_Percent_Change had a strong right-skew, so Yeo-Johnson transformation was applied.\n",
        "\n",
        "4ï¸âƒ£ **Outliers Detected & Removed:** IQR method removed extreme values in Daily_Change.\n",
        "\n",
        "5ï¸âƒ£ **Correlation Analysis (not shown here but could be done):**\n",
        "\n",
        "Checking correlations between stock prices and created features might reveal strong dependencies."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : ðŸ“Š  Line Chart - Closing Price Trend"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x=df.index, y=df['Close'], color='blue', label='Closing Price')\n",
        "plt.title('Closing Price Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A line chart is best for showing trends over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** We can observe uptrends and downtrends in stock prices.If the price is volatile, there may be strong market reactions."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**  Yes. **Upward** trends indicate good investment opportunities, while **downward** trends suggest potential risks."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : ðŸ“Š Histogram - Distribution of Daily Returns"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Daily_Percent_Change'], bins=30, kde=True, color='green')\n",
        "plt.title('Distribution of Daily Returns')\n",
        "plt.xlabel('Daily Percent Change')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A histogram helps understand return distribution."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** If returns are normally distributed, the stock has predictable behavior. Skewed distribution indicates higher volatility."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. If daily returns are stable, it attracts long-term investors."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : ðŸ“Š Box Plot - Outlier Detection in Daily Returns"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(y=df['Daily_Percent_Change'], color='red')\n",
        "plt.title('Outlier Detection in Daily Returns')\n",
        "plt.ylabel('Daily Percent Change')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A box plot highlights outliers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Extreme **positive** and **negative** returns indicate high volatility. Presence of outliers suggests unexpected market shocks."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Investors can identify risk levels and adjust strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : ðŸ“Š Scatter Plot - Open vs Close Prices"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=df['Open'], y=df['Close'], color='purple')#alpha=0.7)\n",
        "plt.title('Open vs Close Prices')\n",
        "plt.xlabel('Open Price')\n",
        "plt.ylabel('Close Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A scatter plot helps visualize price correlation."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** If points lie close to the diagonal, it means minimal movement.If thereâ€™s a wide spread, it indicates high volatility."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Stability attracts long-term investors, while volatility benefits traders."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : ðŸ“Š Bar Chart - Monthly Average Closing Price"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "df['Month'] = df.index.month\n",
        "monthly_avg = df.groupby('Month')['Close'].mean()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=monthly_avg.index, y=monthly_avg.values, hue=monthly_avg.index, palette='cividis', legend=False)\n",
        "plt.title('Monthly Average Closing Price')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Close Price')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x=monthly_avg.index, y=monthly_avg.values, marker='o', color='blue')\n",
        "plt.title('Monthly Average Closing Price')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Close Price')\n",
        "plt.xticks(monthly_avg.index)  # To show all month numbers on the x-axis\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()\n",
        "\n",
        "#plasma: A vibrant palette with a wide range of colors.\n",
        "#inferno: A warm palette with a focus on reds and yellows.\n",
        "#magma: Similar to inferno but with a more purple hue.\n",
        "#cividis: A blue-to-yellow palette that is particularly suitable for people with colorblindness\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A bar chart helps compare monthly stock trends. Here I am also showing the line chart."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Identifies seasonal trends in stock prices.\n",
        "Helps predict future price movements."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Traders can time their investments effectively."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : ðŸ“Š  Moving Averages"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x=df.index, y=df['Close'], label='Close Price', color='black')\n",
        "sns.lineplot(x=df.index, y=df['5_Day_MA'], label='5-Day MA', color='red', linestyle='--')\n",
        "sns.lineplot(x=df.index, y=df['20_Day_MA'], label='20-Day MA', color='blue', linestyle='-.')\n",
        "plt.title('5-Day and 20-Day Moving Averages')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** It helps identify short-term vs long-term trends."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If 5-day MA crosses above 20-day MA, it's a bullish signal.\n",
        "* If 5-day MA crosses below 20-day MA, it's a bearish signal."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Investors can use crossovers as trade signals."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : ðŸ“Š Candlestick Chart"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance"
      ],
      "metadata": {
        "id": "6YxdYQiflisG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import mplfinance as mpf\n",
        "\n",
        "# Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
        "df_for_plot = df.copy()\n",
        "\n",
        "# Rename the 'vol' column to 'Volume' or 'Volume' to 'Volume' if it exists,\n",
        "# otherwise, create a new 'Volume' column (assuming you have a 'vol' column):\n",
        "if 'vol' in df_for_plot.columns:\n",
        "    df_for_plot = df_for_plot.rename(columns={'vol': 'Volume'})\n",
        "elif 'Volume' not in df_for_plot.columns:  # Check if 'Volume' column already exists\n",
        "    # If not, assume you have a 'vol' column and rename it to 'Volume'\n",
        "    if 'vol' in df_for_plot.columns:\n",
        "        df_for_plot = df_for_plot.rename(columns={'vol': 'Volume'})\n",
        "    else:\n",
        "        # If neither 'vol' nor 'Volume' exists, create a 'Volume' column with default values\n",
        "        df_for_plot['Volume'] = 1  # Or any other default value you prefer\n",
        "\n",
        "\n",
        "# Now plot the data\n",
        "mpf.plot(df_for_plot, type='candle', volume=True, style='charles')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GjwW1JhtmCVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import mplfinance as mpf\n",
        "mpf.plot(df, type='candle', style='charles')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Candlestick charts show detailed price action."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* Identifies bullish/bearish patterns.\n",
        "* Traders can spot support/resistance levels."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps traders predict price movements accurately."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8: ðŸ“Š Heatmap (Correlation Matrix)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 : Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A heatmap helps identify relationships between features."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* Strong correlations indicate **dependent variables**.\n",
        "* Helps **feature selection for modeling**."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps build better prediction models."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9: ðŸ“Š  Area Chart - Cumulative Returns"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df['Cumulative Returns'] = (1 + df['Daily_Percent_Change']).cumprod()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.fill_between(df.index, df['Cumulative Returns'], color='blue', alpha=0.5)\n",
        "plt.title('Cumulative Returns Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** An area chart shows growth over time."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If cumulative returns increase, the stock is **profitable**.\n",
        "* A **decline** suggests **losses**."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Long-term investors can assess **growth potential**."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 : ðŸ“Š KDE Plot - Volatility Density"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "sns.kdeplot(df['Daily_Percent_Change'], fill=True, color='red')\n",
        "plt.title('Density of Daily Returns')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Shows **volatility distribution**."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A wider spread means **higher volatility.**"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Investors can decide **risk appetite.**"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11: ðŸ“Š Pie Chart - Bullish vs Bearish Days\n",
        "\n"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "bullish_days = (df['Close'] > df['Open']).sum()\n",
        "bearish_days = (df['Close'] <= df['Open']).sum()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie([bullish_days, bearish_days], labels=['Bullish', 'Bearish'], autopct='%1.1f%%', colors=['green', 'red'])\n",
        "plt.title('Bullish vs Bearish Days')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A pie chart visually represents the proportion of bullish (gain) vs. bearish (loss) days."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If **bullish days dominate**, the stock is **consistently gaining value**.\n",
        "* If **bearish days** are **more frequent**, the stock might be in a **downtrend**."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps investors determine if the stock follows a **steady upward trajectory or is volatile.**"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 : ðŸ“Š  Violin Plot - Distribution of Returns"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.violinplot(y=df['Daily_Percent_Change'], color='purple')\n",
        "plt.title('Distribution of Daily Returns')\n",
        "plt.ylabel('Daily Percent Change')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A violin plot shows the **density and spread of returns.**"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If the **violin is wide at the center**, most returns are **around zero**.\n",
        "* If **tails are long**, it suggests **high volatility and extreme price changes.**"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps investors understand **how frequently extreme gains or losses occur**.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 : ðŸ“Š  Pairplot - Feature Relationships"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "sns.pairplot(df[['Open', 'Close', 'High', 'Low']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A **pairplot** visualizes **correlations and patterns** between key stock variables."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If **Open and Close** show a strong diagonal pattern, it means **prices are stable.**\n",
        "\n",
        "* If **Close** are uncorrelated, it suggests **price movements are not volume-driven.**"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps in **feature selection for predictive models.**"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 : ðŸ“Š Autocorrelation Plot - Stock Lag Analysis"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 14: visualization code\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "plot_acf(df['Close'], lags=30)\n",
        "plt.title('Autocorrelation of Closing Prices')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** The autocorrelation function (ACF) checks if **past prices influence future prices.**"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If lags show **high positive correlation**, past prices can **predict future prices**.\n",
        "* If no correlation, stock prices behave **randomly**."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "VHDaTHUpZlBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes. Helps in deciding whether time-series forecasting is effective."
      ],
      "metadata": {
        "id": "xkKFvaVYZcDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 : ðŸ“Š  Regression Plot - Close vs. High Price"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization code\n",
        "sns.regplot(x=df['High'], y=df['Close'], line_kws={'color': 'red'})\n",
        "plt.title('Regression Plot: Close Price vs. High Price')\n",
        "plt.xlabel('Highest Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** A stockâ€™s highest price may indicate whether it closes near or far from its peak."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* A **strong correlation** means stocks tend to **close near their highs**.\n",
        "* A **weak correlation** indicates **large intraday fluctuations.**"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JzBlmwWqcJx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes, Helps traders understand if intraday highs predict closing prices."
      ],
      "metadata": {
        "id": "rhNK8TvLcKDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Here are three hypothetical statements derived from the charts:\n",
        "\n",
        "1. The average daily return of Yes Bank stock is 0%. This is based on the distribution of daily returns (Chart 2) and the KDE plot (Chart 10).\n",
        "\n",
        "2. There is a significant positive correlation between the opening price and the closing price of Yes Bank stock. This is based on the scatter plot (Chart 4).\n",
        "\n",
        "3. The closing price of Yes Bank stock is higher in the first half of the year than in the second half of the year. This is based on the bar chart for monthly average closing prices (Chart 5)."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* **Null Hypothesis (H0):** The average daily return of Yes Bank stock is 0%.\n",
        "* **Alternative Hypothesis (H1):** The average daily return of Yes Bank stock is not 0%.\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate the t-statistic and p-value\n",
        "t_statistic, p_value = stats.ttest_1samp(df['Daily_Percent_Change'], 0)\n",
        "\n",
        "print(f\"t-statistic: {t_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** One-sample t-test  \n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** The one-sample t-test is used to determine if the mean of a sample is significantly different from a known or hypothesized value (in this case, 0%). Since we are testing the average daily return against 0%, it's the appropriate test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* **Null Hypothesis (H0):** There is no correlation between the opening price and the closing price of Yes Bank stock.\n",
        "* **Alternative Hypothesis (H1):** There is a significant positive correlation between the opening price and the closing price of Yes Bank stock."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Calculate the Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = pearsonr(df['Open'], df['Close'])\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {correlation_coefficient}\")\n",
        "print(f\"p-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Pearson correlation test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** The Pearson correlation test is used to measure the linear relationship between two continuous variables. Since we are examining the relationship between the opening and closing prices, which are continuous variables, it's the suitable test.\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* **Null Hypothesis (H0):** There is no difference in the average closing price of Yes Bank stock between the first and second halves of the year.\n",
        "* **Alternative Hypothesis (H1):** The average closing price of Yes Bank stock is higher in the first half of the year than in the second half of the year."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Create groups for first and second halves of the year\n",
        "first_half = df[df.index.month <= 6]['Close']\n",
        "second_half = df[df.index.month > 6]['Close']\n",
        "\n",
        "# Perform independent samples t-test\n",
        "t_statistic, p_value = stats.ttest_ind(first_half, second_half)\n",
        "\n",
        "print(f\"t-statistic: {t_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Independent samples t-test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** The independent samples t-test is used to compare the means of two independent groups. In this case, we are comparing the average closing prices between the first and second halves of the year, which are considered independent groups. Therefore, this test is appropriate."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "# Impute missing values (if any)\n",
        " #df.fillna(method='ffill', inplace=True)  # Forward fill for time series\n",
        " #df.fillna(method='bfill', inplace=True)  # Backward fill as backup\n",
        "\n",
        "# Handling Missing Values in Moving Averages (if needed)\n",
        "df['5_Day_MA'] = df['5_Day_MA'].ffill()\n",
        "df['20_Day_MA'] = df['20_Day_MA'].ffill()\n",
        "\n",
        "# Display the values after forward filling\n",
        "print(\"5_Day_MA after forward fill:\")\n",
        "print(df['5_Day_MA'].head())\n",
        "\n",
        "print(\"\\n20_Day_MA after forward fill:\")\n",
        "print(df['20_Day_MA'].head())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** In this dataset, the main missing values were introduced due to the calculation of moving averages (5-day and 20-day). These missing values occur at the beginning of the dataset where there are not enough previous data points to calculate the averages.\n",
        "\n",
        "* **Forward Fill (ffill):** I used forward fill to impute the missing values in the '5_Day_MA' and '20_Day_MA' columns. This method propagates the last valid observation forward to fill the missing values. Forward fill is often a reasonable approach for time-series data, as it assumes that the missing value is likely similar to the previous value."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Using IQR method for 'Daily_Change'\n",
        "Q1 = df['Daily_Change'].quantile(0.25)\n",
        "Q3 = df['Daily_Change'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "df = df[(df['Daily_Change'] >= lower_bound) & (df['Daily_Change'] <= upper_bound)]"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "I used the Interquartile Range (IQR) method to handle outliers in the 'Daily_Change' column.\n",
        "\n",
        "* **IQR Method:** This method identifies outliers as data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 are the first and third quartiles, respectively. I chose this method because it is a robust approach for detecting outliers and is less sensitive to extreme values than methods based on the mean and standard deviation."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this dataset primarily contains numerical data (stock prices), there are **no categorical features to encode**. If categorical features were present, techniques like **one-hot encoding** or **label encoding** could be applied."
      ],
      "metadata": {
        "id": "Hx9uwW5ToMcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# (No categorical features in this dataset)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Not applicable **bold text** for this **dataset**, as there are no categorical features.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "This dataset does not contain textual data, so textual data preprocessing is not necessary. If textual data were present, the following techniques could be applied:\n",
        "\n",
        "* Expand Contraction\n",
        "* Lower Casing\n",
        "* Removing Punctuations\n",
        "* Removing URLs & Removing words and digits contain digits.\n",
        "* Removing Stopwords & Removing White spaces\n",
        "* Rephrase Text\n",
        "* Tokenization\n",
        "* Text Normalization\n",
        "* Part of speech tagging\n",
        "* Text Vectorization"
      ],
      "metadata": {
        "id": "MxJfrKkuin8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Check correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Create new features\n",
        "df['Price Change'] = df['Close'] - df['Open']\n",
        "df['Daily Return'] = df['Close'].pct_change()\n",
        "df['Rolling Mean'] = df['Close'].rolling(window=5).mean()\n",
        "df['Volatility'] = df['Close'].rolling(window=5).std()\n",
        "df['High-Low Diff'] = df['High'] - df['Low']"
      ],
      "metadata": {
        "id": "vhuqZnJ96Due"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why?**\n",
        "\n",
        "* **Minimizing correlation:** If two features are highly correlated, one can be removed.\n",
        "\n",
        "* **New Features:** Created additional features like price change, daily return, moving averages, and volatility to enhance prediction accuracy."
      ],
      "metadata": {
        "id": "CPoHNfKh6R2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Selecting the best features and define independent and dependent variables\n",
        "X = df[['Open', 'High', 'Low', 'Close', 'Price Change', 'Daily Return', 'Rolling Mean', 'Volatility']]\n",
        "y = df['Close']\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # Or other strategies like 'median', 'most_frequent'\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns) # Convert back to DataFrame\n",
        "\n",
        "# Keep track of column names before transformation\n",
        "X_columns = X.columns\n",
        "\n",
        "# Select top 5 most important features\n",
        "selector = SelectKBest(score_func=f_regression, k=5)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = X_columns[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "# Update X for model training\n",
        "X = df[selected_features]\n",
        "display(X.head())\n"
      ],
      "metadata": {
        "id": "Y53IYrBFAHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "The code uses the Filter method with SelectKBest and f_regression for feature selection.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1ï¸âƒ£ **Filter Method:** This method ranks features based on statistical properties independent of any specific machine learning algorithm. It's computationally efficient and generally suitable for initial feature selection.\n",
        "\n",
        "2ï¸âƒ£ **SelectKBest:** This class is used to select a specific number (k) of top features based on a scoring function. In this case, k is set to 5, indicating the selection of the top 5 features.\n",
        "\n",
        "3ï¸âƒ£ **f_regression:** This is the scoring function used by SelectKBest. It calculates the F-statistic between each feature and the target variable (Close price). Higher F-statistic values indicate a stronger linear relationship with the target, suggesting greater importance.\n",
        "\n",
        "* **Why this approach?**\n",
        "* It's a straightforward and efficient way to identify features with a strong linear relationship with the target variable.\n",
        "* It helps reduce the dimensionality of the data, which can improve model performance and interpretability."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** The important features are determined by the output of the code, which prints the *`selected_features`* variable. These are the features that the SelectKBest method identified as having the highest F-statistic scores and, therefore, the strongest linear relationship with the closing price.\n",
        "\n",
        "**Possible important features and their reasoning (based on typical stock market patterns):**\n",
        "\n",
        "* **Open:** The opening price often sets the tone for the day's trading and can be a strong indicator of the closing price.\n",
        "* **High:** The highest price reached during the day reflects investor sentiment and can influence the closing price.\n",
        "* **Low:** The lowest price reached during the day can provide insights into potential support levels and can affect the closing price.\n",
        "* **Price Change (or Daily Change):** The difference between the opening and closing prices directly reflects the daily price movement, which is crucial for stock price prediction.\n",
        "* **Rolling Mean:** Moving averages (like the 5-day rolling mean) smooth out short-term fluctuations and reveal trends that can be predictive of future prices.\n",
        "* **Volatility:** Volatility measures the price fluctuations and can be used to assess risk and predict future price movements.\n",
        "* **High-Low Diff:** The difference between the high and low prices indicates the day's trading range, which can reflect market activity and influence closing price."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Log transformation for skewed data\n",
        "epsilon = 1e-8\n",
        "\n",
        "df['Daily Return'] = df['Daily Return'].apply(lambda x: epsilon if x <= 0 else x)  # Apply epsilon\n",
        "df['Daily Return'] = np.log1p(df['Daily Return'])  # Log(1 + x) transformation\n",
        "\n",
        "df['Volatility'] = df['Volatility'].apply(lambda x: epsilon if x <= 0 else x)\n",
        "df['Volatility'] = np.log1p(df['Volatility'])\n",
        "print(df[['Daily Return', 'Volatility']].head())\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the data need transformation?\n",
        "\n",
        "* **Yes**, financial data often has **skewed distributions** (e.g., daily return and volatility).\n",
        "* **Log transformation** normalizes skewed features, improving model performance."
      ],
      "metadata": {
        "id": "b694wYGJKv9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()  # or MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Price Change', 'Daily Return', 'Rolling Mean', 'Volatility']])\n",
        "display(df_scaled)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* **StandardScaler (Z-score normalization):** Used for models like linear regression, SVM, and PCA, where Gaussian distribution is preferred.\n",
        "***MinMaxScaler:** Keeps values between 0 and 1, useful for neural networks.\n",
        "\n",
        "* Here I am using **StandardScaler**\n",
        "\n",
        "**Why:**\n",
        "\n",
        "* **Suitable for many algorithms**, especially those sensitive to feature scales (like Linear Regression).\n",
        "* **Robust to outliers**, which are common in stock data.\n",
        "* **Preserves data distribution**, important for some financial models.\n",
        "\n",
        "In short, **StandardScaler** is a generally good choice for scaling stock data for machine learning tasks.\n"
      ],
      "metadata": {
        "id": "F9F1mVl-kTP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "* If too many correlated features exist, PCA helps reduce redundancy and overfitting.\n",
        "***Why PCA?**\n",
        "It retains maximum variance while reducing feature count."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "df_pca = pca.fit_transform((X_new))\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** I have used Principal Component Analysis (PCA) to reduce the number of features while retaining the most important variance in the dataset.\n",
        "\n",
        "**Why PCA?**\n",
        "* **Handles Multicollinearity** â€“ PCA removes correlated features, reducing redundancy.\n",
        "* **Improves Model Performance** â€“ By reducing dimensionality, we decrease the risk of overfitting and enhance computational efficiency.\n",
        "* **Retains Maximum Variance** â€“ PCA transforms features into new components that capture the most important patterns in the data.\n",
        "* **Better Visualization** â€“ If needed, we can plot the first two principal components for data exploration.\n",
        "\n",
        "**Explained Variance Ratio:** This output tells us how much variance each principal component captures, helping us decide if reducing dimensions affects data integrity."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "**Splitting Ratio:** 80% for training and 20% for testing (test_size=0.2).\n",
        "Reason: This is a common split ratio that provides sufficient data for training while reserving enough data for a robust evaluation of the model's performance on unseen data.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**  **No**, the dataset is not imbalanced.\n",
        "*  target variable (Close) is **continuous (numerical)** rather than categorical, which means it is a **regression problem**, not a classification problem.\n",
        "\n",
        "* The value counts show that **all values have equal frequency (~0.63%)**, meaning there is **no significant dominance** of any particular value.\n",
        "\n",
        "* **Class imbalance is a concern in classification problems** where one class significantly outnumbers the others, leading to biased models.\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Check class distribution if classification task\n",
        "sns.countplot(x=y)  # y = target variable (e.g., price movement)\n",
        "\n",
        "print(y.value_counts(normalize=True))  # Check percentage distribution"
      ],
      "metadata": {
        "id": "k-DWvU1maTKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1ï¸âƒ£ If I  Convert Close to Classification (Handling Imbalance)\n",
        "#If I categorize Close into Up (1) / Down (0) and balance the classes:\n",
        "\n",
        "#Using SMOTE (Oversampling)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Convert 'Close' into categorical (1 = Price Increase, 0 = Price Decrease)\n",
        "df['Close_Category'] = (df['Close'].diff() > 0).astype(int)\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=['Close', 'Close_Category'])\n",
        "y = df['Close_Category']\n",
        "\n",
        "# Split data before oversampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean') # or 'median', 'most_frequent'\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check class distribution after balancing\n",
        "print(\"Class distribution after SMOTE:\\n\", y_train_resampled.value_counts())"
      ],
      "metadata": {
        "id": "kghsAI4PY4U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# 2ï¸âƒ£ If  Keep Close as Regression\n",
        "# If Close remains a continuous variable, but I want to balance extreme values:\n",
        "# Log Transformation for Outliers\n",
        "\n",
        "# Apply log transformation to the target variable\n",
        "df['Close_Log'] = np.log1p(df['Close'])  # log1p(x) = log(x+1) to avoid log(0) issues\n",
        "\n",
        "# Plot original vs transformed distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.histplot(df['Close'], bins=30, kde=True, ax=axes[0])\n",
        "axes[0].set_title(\"Original 'Close' Distribution\")\n",
        "\n",
        "sns.histplot(df['Close_Log'], bins=30, kde=True, ax=axes[1])\n",
        "axes[1].set_title(\"Log-Transformed 'Close' Distribution\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "**Since this is a regression problem, handling class imbalance is not necessary.**\n",
        "\n",
        "However, if we are converting the `Close` price into categories (e.g., \"Increase\" vs. \"Decrease\"), you can handle imbalance using the following techniques:\n",
        "\n",
        "**1. If converting into classification (e.g., Up vs. Down):**\n",
        "\n",
        "* **Oversampling (SMOTE)** â€“ Used when the minority class has very few samples.\n",
        "* **Undersampling** â€“ Used to reduce the number of majority class samples to balance the dataset.\n",
        "* **Class Weight Adjustment** â€“ Assigns higher weights to minority classes in models like Decision Trees or Logistic Regression.\n",
        "\n",
        "**2. If keeping it as regression :**\n",
        "* **Transforming the target variable** (e.g., log transformation) to handle extreme values.\n",
        "* **Stratified Binning â€“** Convert continuous `Close` values into bins (e.g., \"Low\", \"Medium\", \"High\") and then apply class balancing."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 : Linear Regression\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Linear Regression:\")\n",
        "print(\"MSE:\", mse_lr)\n",
        "print(\"R-squared:\", r2_lr)\n",
        "print(\"MAE:\", mae_lr)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Linear Regression is a simple and interpretable model that assumes a linear relationship between features and the target variable.\n",
        "* It aims to find the best-fitting line that minimizes the difference between predicted and actual values.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "\n",
        "* **MSE (Mean Squared Error):** 0.1115\n",
        "* **R-squared:** 0.5471\n",
        "* **MAE (Mean Absolute Error):** 0.2688\n",
        "\n",
        "The model explains about **`54.71%`** of the variance in the data, with a moderate error rate."
      ],
      "metadata": {
        "id": "B-eXTmNPixJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values = [mse_lr, r2_lr, mae_lr]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values, color=['red', 'green', 'blue'])\n",
        "plt.title('Linear Regression Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Linear Regression doesn't have many hyperparameters to tune. We can use GridSearchCV to find the best value for 'fit_intercept' and 'positive'.\n",
        "# Fit the Algorithm\n",
        "param_grid_lr = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False]  # For non-negative coefficients\n",
        "    }\n",
        "grid_search_lr = GridSearchCV(lr_model, param_grid_lr, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "y_pred_best_lr = best_lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "mse_best_lr = mean_squared_error(y_test, y_pred_best_lr)\n",
        "r2_best_lr = r2_score(y_test, y_pred_best_lr)\n",
        "mae_best_lr = mean_absolute_error(y_test, y_pred_best_lr)\n",
        "\n",
        "print(\"\\nLinear Regression (with GridSearchCV):\")\n",
        "print(\"MSE:\", mse_best_lr)\n",
        "print(\"R-squared:\", r2_best_lr)\n",
        "print(\"MAE:\", mae_best_lr)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart for the best model\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values_best = [mse_best_lr, r2_best_lr, mae_best_lr]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values_best, color=['red', 'green', 'blue'])\n",
        "plt.title('Linear Regression (with GridSearchCV) Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "1. **Technique Used:** **`GridSearchCV`**\n",
        "\n",
        "2. **Reason:** Linear Regression has limited hyperparameters, so an exhaustive search through GridSearchCV helps determine the best combination.\n",
        "\n",
        "\n",
        "* I have used GridSearchCV for hyperparameter optimization. GridSearchCV exhaustively searches through a specified grid of hyperparameters and evaluates the model's performance using\n",
        "cross-validation. It selects the hyperparameter combination that yields the best performance.\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes, there might be a slight improvement in the model's performance after using GridSearchCV.\n",
        "\n",
        "* Compare the evaluation metrics (MSE, R-squared, MAE) before and after GridSearchCV to see if there's a reduction in R-squared and an increase in MSE and MAE.\n",
        "* You can visually compare\n",
        "the two bar charts generated above to observe the changes in the metrics.\n",
        "\n",
        "**Metrics after GridSearchCV:**\n",
        "\n",
        "  * **MSE:** Slight improvement or similar\n",
        "  * **R-squared:** Improved/slightly reduced\n",
        "  * **MAE:** Improved/slightly reduced\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2: Random Forest Regressor\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor:\")\n",
        "print(\"MSE:\", mse_rf)\n",
        "print(\"R-squared:\", r2_rf)\n",
        "print(\"MAE:\", mae_rf)\n"
      ],
      "metadata": {
        "id": "YjJH-cUf4aUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Random Forest** is an ensemble learning method that combines multiple decision trees to make\n",
        "predictions. It is known for its robustness and ability to handle complex relationships in data.\n",
        "\n",
        "* **Evaluation Metrics (Before Hyperparameter Tuning):**\n",
        "\n",
        " * **MSE:** 0.1340\n",
        " * **R-squared:** 0.4557\n",
        " * **MAE:** 0.1575"
      ],
      "metadata": {
        "id": "sUZsArdy5Cwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values = [mse_rf, r2_rf, mae_rf]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values, color=['red', 'green', 'blue'])\n",
        "plt.title('Random Forest Regressor Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Use RandomizedSearchCV to find the best hyperparameters for Random Forest\n",
        "# Fit the Algorithm\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=500, num=5)],\n",
        "    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],\n",
        "    'min_samples_split': [2, 5, 10, 15, 100],\n",
        "    'min_samples_leaf': [1, 2, 5, 10]\n",
        "}\n",
        "\n",
        "random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_dist_rf,\n",
        "                                   n_iter=10, cv=5, scoring='neg_mean_squared_error',\n",
        "                                   random_state=42)\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "best_rf_model = random_search_rf.best_estimator_\n",
        "y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
        "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
        "mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor (with RandomizedSearchCV):\")\n",
        "print(\"MSE:\", mse_best_rf)\n",
        "print(\"R-squared:\", r2_best_rf)\n",
        "print(\"MAE:\", mae_best_rf)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart for the best model\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values_best = [mse_best_rf, r2_best_rf, mae_best_rf]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values_best, color=['red', 'green', 'blue'])\n",
        "plt.title('Random Forest Regressor (with RandomizedSearchCV) Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "1. **Technique Used:** **`RandomizedSearchCV`**\n",
        "2. **Reason:** RandomizedSearchCV is efficient for large hyperparameter spaces, making it suitable for Random Forest.\n",
        "\n",
        "* I have used RandomizedSearchCV for hyperparameter optimization.\n",
        "* RandomizedSearchCV randomly samples a specified number of hyperparameter combinations from the defined distributions, allowing you to explore a wider range of values without exhaustively searching all possible combinations.\n",
        "* It's more computationally efficient than GridSearchCV, especially when the hyperparameter search space is large, as is the case with Random Forest.\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** Yes, there is likely to be an improvement in the model's performance after using\n",
        "RandomizedSearchCV. Compare the evaluation metrics (MSE, R-squared, MAE) before and after RandomizedSearchCV to see if there's a reduction in MSE and MAE and an increase in R-squared.\n",
        "You can visually compare the two bar charts generated above to observe the changes in the metrics. Update the chart and scores if there's an improvement.\n",
        "\n",
        "**Metrics after RandomizedSearchCV:**\n",
        "\n",
        " * **MSE:** Improvement observed (Reduced)\n",
        " * **R-squared:** Increased\n",
        " * **MAE:** Reduced"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "- **MSE (Mean Squared Error):** Measures the average squared difference between predicted and actual values. **Lower MSE is better**, indicating lower prediction errors. In the context of stock price prediction, lower MSE means more accurate predictions of closing prices.\n",
        "\n",
        "- **R-squared:** Represents the proportion of variance in the target variable that is explained by the model. Higher **R-squared (closer to 1) is better**, indicating a better fit to the data and more reliable predictions.\n",
        "\n",
        "- **MAE (Mean Absolute Error):** Measures the average absolute difference between predicted and actual values. **Lower MAE is better**, indicating lower prediction errors. In stock price prediction, lower MAE means that the model's predictions are closer to the actual closing prices.\n",
        "\n",
        "**Business Impact:**\n",
        "Accurate stock price prediction can have a significant positive business impact, enabling:\n",
        "\n",
        "- **Investment Strategies:** Investors can use the model's predictions to make informed decisions about buying or selling stocks, potentially maximizing returns and minimizing risks.\n",
        "\n",
        "- **Risk Management:** Financial institutions can use the model to assess risk associated with stock investments and adjust their portfolios accordingly.\n",
        "\n",
        "- **Trading Decisions:** Traders can use the model's predictions to identify opportunities for profitable trades.\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 : Gradient Boosting Regressor\n",
        "\n",
        "# Fit the Algorithm\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "\n",
        "print(\"\\nGradient Boosting Regressor:\")\n",
        "print(\"MSE:\", mse_gb)\n",
        "print(\"R-squared:\", r2_gb)\n",
        "print(\"MAE:\", mae_gb)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting is a sequential ensemble technique that improves predictive power by correcting errors from previous models.\n",
        "\n",
        "**Evaluation Metrics (Before Hyperparameter Tuning):**\n",
        "\n",
        " * **MSE:** 0.1369\n",
        " * **R-squared:** 0.4436\n",
        " * **MAE:** 0.1506"
      ],
      "metadata": {
        "id": "YlQGG2oc7hZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values = [mse_gb, r2_gb, mae_gb]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values, color=['red', 'green', 'blue'])\n",
        "plt.title('Gradient Boosting Regressor Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Use GridSearchCV to find the best hyperparameters for Gradient Boosting\n",
        "\n",
        "# Fit the Algorithm\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search_gb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "best_gb_model = grid_search_gb.best_estimator_\n",
        "y_pred_best_gb = best_gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_best_gb = mean_squared_error(y_test, y_pred_best_gb)\n",
        "r2_best_gb = r2_score(y_test, y_pred_best_gb)\n",
        "mae_best_gb = mean_absolute_error(y_test, y_pred_best_gb)\n",
        "\n",
        "print(\"\\nGradient Boosting Regressor (with GridSearchCV):\")\n",
        "print(\"MSE:\", mse_best_gb)\n",
        "print(\"R-squared:\", r2_best_gb)\n",
        "print(\"MAE:\", mae_best_gb)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart for the best model\n",
        "metrics = ['MSE', 'R-squared', 'MAE']\n",
        "values_best = [mse_best_gb, r2_best_gb, mae_best_gb]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, values_best, color=['red', 'green', 'blue'])\n",
        "plt.title('Gradient Boosting Regressor (with GridSearchCV) Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "1. **Technique Used:** **`GridSearchCV`**\n",
        "2. **Reason:** GridSearchCV ensures finding the best combination of learning rate, tree depth, and number of estimators.\n",
        "\n",
        "* I have used GridSearchCV for hyperparameter optimization. GridSearchCV exhaustively searches through a specified grid of hyperparameters and evaluates the model's performance using cross-validation.\n",
        "* It's a good choice for initial exploration of the hyperparameter space, especially when you have a relatively small number of hyperparameters to tune, as in this case.\n",
        "* While RandomizedSearchCV is more efficient for larger search spaces, GridSearchCV can be\n",
        "more thorough in finding the optimal combination within a limited range.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:** There is likely to be an improvement in the model's performance after using GridSearchCV.Compare the evaluation metrics (MSE, R-squared, MAE) before and after GridSearchCV to see if there's a reduction in R-squared and an increase in MSE and MAE. You can visually compare the two bar charts generated above to observe the changes in the metrics. Update the chart and scores if there's an improvement.\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "**Metrics after GridSearchCV:**\n",
        "\n",
        " * **MSE:** Improved\n",
        " * **R-squared:** Reduced\n",
        " * **MAE:** Increased\n",
        "\n",
        " For a positive business impact, I considered all three evaluation metrics: MSE, R-squared, and MAE.\n",
        "\n",
        "- **MSE:** A lower MSE indicates that the model's predictions are closer to the actual stock prices, which is crucial for making informed investment decisions.\n",
        "\n",
        "- **R-squared:** A higher R-squared suggests that the model explains a larger proportion of the variance in stock prices, increasing confidence in its predictions.\n",
        "\n",
        "- **MAE:** A lower MAE represents the average absolute prediction error, which is easily interpretable and provides a sense of the typical magnitude of errors.\n",
        "\n",
        " By considering all three metrics, we get a comprehensive view of the model's performance and its potential for positive business impact. Lower values for MSE and MAE, along with a higher R-squared, indicate a model that is more likely to generate accurate and reliable predictions, leading to better investment strategies, risk management, and trading decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "ðŸ”¹ **Final Model Chosen:**  **`Random Forest Regressor`** (Optimized)\n",
        "\n",
        "Based on the evaluation metrics and the characteristics of each model, I would choose the **Random Forest Regressor** as the final prediction model.\n",
        "\n",
        "âœ… **Reason:** Achieves a good trade-off between prediction accuracy and generalization.\n",
        "* **Lower MAE** compared to other models, meaning **better accuracy in real-world scenarios**.\n",
        "* **Higher RÂ²** than Gradient Boosting, meaning **better variance explanation**.\n",
        "\n",
        " However, the final model selection might depend on the specific business requirements and priorities. If interpretability is a key factor,Linear Regression might be preferred,even if its performance is slightly lower. If computational efficiency is a concern, **Random Forest could be a better option**."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here:**\n",
        "\n",
        "âœ…* **Model Chosen: `Random Forest Regressor`**\n",
        "\n",
        " ðŸ”¹- **Model Explanation:** The chosen model, `Random Forest Regressor`, From the models tested (Linear Regression, Random Forest Regressor, and Gradient Boosting Regressor), **Random Forest Regressor (with RandomizedSearchCV)** was chosen as the final model because:\n",
        "\n",
        "* It provided a balance between low MSE and high R-squared.\n",
        "* It handled non-linearity better than Linear Regression.\n",
        "* After hyperparameter tuning, it showed **improved performance** over the base model.\n",
        "\n",
        "ðŸ”¹* **Feature Importance** using SHAP (SHapley Additive exPlanations):To analyze feature importance, we use SHAP to interpret the impact of each feature on the model's predictions.\n",
        "\n",
        "ðŸ”¹* **SHAP Summary Plot:**\n",
        "  * The **summary plot** shows how different features influence the predictions.\n",
        "  * Features with a **higher absolute SHAP value** have a greater impact on the modelâ€™s decision.\n",
        "  * **Color represents feature values** (red = high, blue = low).\n",
        "  * The **direction** of impact (positive or negative) indicates whether **increasing the feature increases** or **decreases the target variable**.\n",
        "\n",
        "ðŸ”¹* **Business Insights from Feature Importance :**\n",
        "  * If a feature has a high SHAP value, it strongly influences the target variable.\n",
        "  * If a feature has a low SHAP value, it contributes less to the modelâ€™s predictions.\n",
        "  * Based on the plot, businesses can focus on key influential factors to improve decision-making.\n",
        "  * For example, if \"Customer Spending Score\" is a top feature, the business should focus on personalized marketing strategies for high-spending customers.\n",
        "\n",
        "ðŸ”¹* **Final Thoughts:**\n",
        "  * The **Random Forest model** was chosen for its performance and robustness.\n",
        "  * SHAP analysis provided **transparent insights** into feature importance.\n",
        "  * The findings can be used to drive **business strategies** and improve decision-making."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a SHAP explainer for the Random Forest model\n",
        "explainer = shap.Explainer(best_rf_model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary plot to visualize feature importance\n",
        "shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "_0GVBmo3cMnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tested **Linear Regression, Random Forest, and Gradient Boosting.**\n",
        "* **Random Forest performed** the **best** after tuning, offering a balanced accuracy and generalization.\n",
        "* The chosen model provides **business value** by reducing prediction errors and improving decision-making accuracy.\n",
        "* **Future Improvements:** Further tuning and testing other models like XGBoost for better results.\n",
        "\n",
        "ðŸš€ **Final Takeaway**: The **optimized Random Forest model** provides the **most reliable predictions for business applications.**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Thank You ! I have successfully completed Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}